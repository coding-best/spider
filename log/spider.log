ERROR: 09-16 15:01:42: spider_main.py:68 -- Conf file:  is not exist
ERROR: 09-16 15:05:41: spider_main.py:68 -- Conf file:  is not exist
INFO: 09-16 15:07:20: spider_crawl.py:44 -- Add the seeds url "['http://task.csdn.net/']"
ERROR: 09-16 15:09:14: spider_main.py:68 -- Conf file:  is not exist
ERROR: 09-16 15:10:17: spider_main.py:68 -- Conf file:  is not exist
ERROR: 09-16 15:10:59: spider_main.py:68 -- Conf file:  is not exist
ERROR: 09-16 15:11:26: spider_main.py:68 -- Conf file:  is not exist
ERROR: 09-16 15:11:38: spider_main.py:68 -- Conf file:  is not exist
ERROR: 09-16 15:12:02: spider_main.py:68 -- Conf file:  is not exist
ERROR: 09-16 15:12:22: spider_main.py:68 -- Conf file:  is not exist
INFO: 09-16 15:13:14: spider_crawl.py:44 -- Add the seeds url "['http://task.csdn.net/']"
INFO: 09-16 15:14:15: spider_crawl.py:44 -- Add the seeds url "['http://task.csdn.net/']"
INFO: 09-16 15:14:15: spider_crawl.py:55 -- Begin Crawling
INFO: 09-16 15:14:15: spider_crawl.py:73 -- crawling the urls : http://task.csdn.net/, current_depth=0
INFO: 09-16 15:14:16: spider_crawl.py:101 -- get http://task.csdn.net/ content successfully
INFO: 09-16 15:14:16: spider_crawl.py:78 -- get 2 new links
INFO: 09-16 15:14:16: spider_crawl.py:73 -- crawling the urls : http://task.csdn.net/m/task/home/lists/3, current_depth=1
INFO: 09-16 15:14:17: spider_crawl.py:101 -- get http://task.csdn.net/m/task/home/lists/3 content successfully
INFO: 09-16 15:14:17: spider_crawl.py:78 -- get 4 new links
INFO: 09-16 15:14:17: spider_crawl.py:73 -- crawling the urls : http://task.csdn.net/m/task/home/lists/, current_depth=1
INFO: 09-16 15:14:19: spider_crawl.py:101 -- get http://task.csdn.net/m/task/home/lists/ content successfully
INFO: 09-16 15:14:19: spider_crawl.py:78 -- get 2 new links
INFO: 09-16 15:14:19: spider_crawl.py:90 -- this thread crawling done!!! max_depth=1
INFO: 09-16 15:22:21: spider_crawl.py:44 -- Add the seeds url "['http://task.csdn.net/']"
INFO: 09-16 15:22:39: spider_crawl.py:44 -- Add the seeds url "['http://task.csdn.net/']"
INFO: 09-16 15:22:39: spider_crawl.py:74 -- crawling the urls : http://task.csdn.net/, current_depth=0
INFO: 09-16 15:22:41: spider_crawl.py:102 -- get http://task.csdn.net/ content successfully
INFO: 09-16 15:22:41: spider_crawl.py:79 -- get 2 new links
INFO: 09-16 15:22:41: spider_crawl.py:74 -- crawling the urls : http://task.csdn.net/m/task/home/lists/3, current_depth=1
INFO: 09-16 15:22:42: spider_crawl.py:102 -- get http://task.csdn.net/m/task/home/lists/3 content successfully
INFO: 09-16 15:22:42: spider_crawl.py:79 -- get 4 new links
INFO: 09-16 15:22:42: spider_crawl.py:74 -- crawling the urls : http://task.csdn.net/m/task/home/lists/, current_depth=1
INFO: 09-16 15:22:43: spider_crawl.py:102 -- get http://task.csdn.net/m/task/home/lists/ content successfully
INFO: 09-16 15:22:43: spider_crawl.py:79 -- get 2 new links
INFO: 09-16 15:22:43: spider_crawl.py:91 -- this thread crawling done!!! max_depth=1
INFO: 09-16 15:22:43: spider_crawl.py:53 -- Begin Crawling
INFO: 09-16 15:24:40: spider_crawl.py:44 -- Add the seeds url "['http://task.csdn.net/']"
INFO: 09-16 15:24:40: spider_crawl.py:73 -- crawling the urls : http://task.csdn.net/, current_depth=0
INFO: 09-16 15:24:41: spider_crawl.py:101 -- get http://task.csdn.net/ content successfully
INFO: 09-16 15:24:41: spider_crawl.py:78 -- get 2 new links
INFO: 09-16 15:24:41: spider_crawl.py:73 -- crawling the urls : http://task.csdn.net/m/task/home/lists/3, current_depth=1
INFO: 09-16 15:24:42: spider_crawl.py:101 -- get http://task.csdn.net/m/task/home/lists/3 content successfully
INFO: 09-16 15:24:42: spider_crawl.py:78 -- get 4 new links
INFO: 09-16 15:24:42: spider_crawl.py:73 -- crawling the urls : http://task.csdn.net/m/task/home/lists/, current_depth=1
INFO: 09-16 15:24:43: spider_crawl.py:101 -- get http://task.csdn.net/m/task/home/lists/ content successfully
INFO: 09-16 15:24:43: spider_crawl.py:78 -- get 2 new links
INFO: 09-16 15:24:43: spider_crawl.py:90 -- this thread crawling done!!! max_depth=1
INFO: 09-16 15:24:43: spider_crawl.py:53 -- Begin Crawling
INFO: 09-16 15:26:08: spider_crawl.py:44 -- Add the seeds url "['http://task.csdn.net/\n', 'http://www.baidu.com']"
INFO: 09-16 15:26:08: spider_crawl.py:55 -- Begin Crawling
INFO: 09-16 15:26:08: spider_crawl.py:73 -- crawling the urls : http://task.csdn.net/
, current_depth=0
ERROR: 09-16 15:26:09: spider_crawl.py:106 -- get url: http://task.csdn.net/
 faild,status_code = 404
ERROR: 09-16 15:26:09: spider_crawl.py:83 -- get url:http://task.csdn.net/
 html error
INFO: 09-16 15:26:09: spider_crawl.py:90 -- this thread crawling done!!! max_depth=1
INFO: 09-16 15:28:10: spider_crawl.py:44 -- Add the seeds url "['http://task.csdn.net/\n', 'http://www.baidu.com']"
INFO: 09-16 15:28:10: spider_crawl.py:59 -- Begin Crawling
INFO: 09-16 15:28:10: spider_crawl.py:76 -- crawling the urls : http://task.csdn.net/
, current_depth=0
ERROR: 09-16 15:28:11: spider_crawl.py:109 -- get url: http://task.csdn.net/
 faild,status_code = 404
ERROR: 09-16 15:28:11: spider_crawl.py:86 -- get url:http://task.csdn.net/
 html error
INFO: 09-16 15:28:11: spider_crawl.py:93 -- this thread crawling done!!! max_depth=1
INFO: 09-16 15:28:11: spider_crawl.py:59 -- Begin Crawling
INFO: 09-16 15:28:11: spider_crawl.py:76 -- crawling the urls : http://www.baidu.com, current_depth=0
INFO: 09-16 15:28:12: spider_crawl.py:104 -- get http://www.baidu.com content successfully
INFO: 09-16 15:28:13: spider_crawl.py:81 -- get 18 new links
INFO: 09-16 15:28:13: spider_crawl.py:76 -- crawling the urls : http://map.baidu.com, current_depth=1
INFO: 09-16 15:28:15: spider_crawl.py:104 -- get http://map.baidu.com content successfully
INFO: 09-16 15:28:16: spider_crawl.py:81 -- get 12 new links
INFO: 09-16 15:28:16: spider_crawl.py:76 -- crawling the urls : http://zhidao.baidu.com/q?ct=17&pn=0&tn=ikaslist&rn=10&word=&fr=wwwt, current_depth=1
INFO: 09-16 15:28:18: spider_crawl.py:104 -- get http://zhidao.baidu.com/q?ct=17&pn=0&tn=ikaslist&rn=10&word=&fr=wwwt content successfully
INFO: 09-16 15:28:19: spider_crawl.py:81 -- get 14 new links
INFO: 09-16 15:28:19: spider_crawl.py:76 -- crawling the urls : http://www.baidu.com/duty/, current_depth=1
INFO: 09-16 15:28:20: spider_crawl.py:104 -- get http://www.baidu.com/duty/ content successfully
INFO: 09-16 15:28:20: spider_crawl.py:81 -- get 3 new links
INFO: 09-16 15:28:20: spider_crawl.py:76 -- crawling the urls : http://tieba.baidu.com/f?kw=&fr=wwwt, current_depth=1
INFO: 09-16 15:28:25: spider_crawl.py:104 -- get http://tieba.baidu.com/f?kw=&fr=wwwt content successfully
INFO: 09-16 15:28:26: spider_crawl.py:81 -- get 12 new links
INFO: 09-16 15:28:26: spider_crawl.py:76 -- crawling the urls : http://xueshu.baidu.com, current_depth=1
INFO: 09-16 15:28:27: spider_crawl.py:104 -- get http://xueshu.baidu.com content successfully
INFO: 09-16 15:28:27: spider_crawl.py:81 -- get 10 new links
INFO: 09-16 15:28:27: spider_crawl.py:76 -- crawling the urls : https://www.hao123.com, current_depth=1
INFO: 09-16 15:28:29: spider_crawl.py:104 -- get https://www.hao123.com content successfully
INFO: 09-16 15:28:31: spider_crawl.py:81 -- get 349 new links
INFO: 09-16 15:28:31: spider_crawl.py:76 -- crawling the urls : http://wenku.baidu.com/search?word=&lm=0&od=0&ie=utf-8, current_depth=1
INFO: 09-16 15:30:18: spider_crawl.py:44 -- Add the seeds url "['http://task.csdn.net/\n', 'http://www.baidu.com']"
INFO: 09-16 15:30:18: spider_crawl.py:59 -- Begin Crawling
INFO: 09-16 15:30:18: spider_crawl.py:76 -- crawling the urls : http://task.csdn.net/
, current_depth=0
ERROR: 09-16 15:30:20: spider_crawl.py:109 -- get url: http://task.csdn.net/
 faild,status_code = 404
ERROR: 09-16 15:30:20: spider_crawl.py:86 -- get url:http://task.csdn.net/
 html error
INFO: 09-16 15:30:20: spider_crawl.py:93 -- this thread crawling done!!! max_depth=1
INFO: 09-16 15:30:20: spider_crawl.py:59 -- Begin Crawling
INFO: 09-16 15:30:20: spider_crawl.py:76 -- crawling the urls : http://www.baidu.com, current_depth=0
INFO: 09-16 15:30:21: spider_crawl.py:104 -- get http://www.baidu.com content successfully
INFO: 09-16 15:30:22: spider_crawl.py:81 -- get 18 new links
INFO: 09-16 15:30:22: spider_crawl.py:93 -- this thread crawling done!!! max_depth=1
INFO: 09-16 15:31:53: spider_crawl.py:44 -- Add the seeds url "['http://task.csdn.net/\n', 'http://www.baidu.com']"
INFO: 09-16 15:31:53: spider_crawl.py:59 -- Begin Crawling
INFO: 09-16 15:31:53: spider_crawl.py:76 -- crawling the urls : http://task.csdn.net/
, current_depth=0
ERROR: 09-16 15:31:54: spider_crawl.py:109 -- get url: http://task.csdn.net/
 faild,status_code = 404
ERROR: 09-16 15:31:54: spider_crawl.py:86 -- get url:http://task.csdn.net/
 html error
INFO: 09-16 15:31:54: spider_crawl.py:93 -- this thread crawling done!!! max_depth=1
INFO: 09-16 15:32:13: spider_crawl.py:44 -- Add the seeds url "['http://task.csdn.net/\n', 'http://www.baidu.com']"
INFO: 09-16 15:32:13: spider_crawl.py:59 -- Begin Crawling
INFO: 09-16 15:32:13: spider_crawl.py:76 -- crawling the urls : http://task.csdn.net/
, current_depth=0
ERROR: 09-16 15:32:14: spider_crawl.py:109 -- get url: http://task.csdn.net/
 faild,status_code = 404
ERROR: 09-16 15:32:14: spider_crawl.py:86 -- get url:http://task.csdn.net/
 html error
INFO: 09-16 15:32:14: spider_crawl.py:93 -- this thread crawling done!!! max_depth=1
INFO: 09-16 15:32:14: spider_crawl.py:59 -- Begin Crawling
INFO: 09-16 15:32:14: spider_crawl.py:76 -- crawling the urls : http://www.baidu.com, current_depth=0
INFO: 09-16 15:32:16: spider_crawl.py:104 -- get http://www.baidu.com content successfully
INFO: 09-16 15:32:16: spider_crawl.py:81 -- get 18 new links
INFO: 09-16 15:32:16: spider_crawl.py:93 -- this thread crawling done!!! max_depth=1
INFO: 09-16 15:33:48: spider_crawl.py:44 -- Add the seeds url "['http://task.csdn.net/\n', 'http://www.baidu.com']"
INFO: 09-16 15:33:48: spider_crawl.py:59 -- Begin Crawling
INFO: 09-16 15:33:48: spider_crawl.py:77 -- crawling the urls : http://task.csdn.net/
, current_depth=0
ERROR: 09-16 15:33:49: spider_crawl.py:110 -- get url: http://task.csdn.net/
 faild,status_code = 404
ERROR: 09-16 15:33:49: spider_crawl.py:87 -- get url:http://task.csdn.net/
 html error
INFO: 09-16 15:33:49: spider_crawl.py:94 -- this thread crawling done!!! max_depth=1
INFO: 09-16 15:33:49: spider_crawl.py:59 -- Begin Crawling
INFO: 09-16 15:33:49: spider_crawl.py:77 -- crawling the urls : http://www.baidu.com, current_depth=0
INFO: 09-16 15:33:50: spider_crawl.py:105 -- get http://www.baidu.com content successfully
INFO: 09-16 15:33:51: spider_crawl.py:82 -- get 18 new links
INFO: 09-16 15:33:51: spider_crawl.py:94 -- this thread crawling done!!! max_depth=1
INFO: 09-16 15:38:01: spider_crawl.py:44 -- Add the seeds url "['http://www.baidu.com\n', 'http://task.csdn.net/\n']"
INFO: 09-16 15:38:01: spider_crawl.py:59 -- Begin Crawling
INFO: 09-16 15:38:01: spider_crawl.py:76 -- crawling the urls : http://www.baidu.com
, current_depth=0
ERROR: 09-16 15:38:05: spider_crawl.py:111 -- can not connect to the http://www.baidu.com
,info:HTTPConnectionPool(host='www.baidu.com%0a', port=80): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x10aecde90>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known',))
ERROR: 09-16 15:38:05: spider_crawl.py:86 -- get url:http://www.baidu.com
 html error
INFO: 09-16 15:38:05: spider_crawl.py:93 -- this thread crawling done!!! max_depth=1
INFO: 09-16 15:38:05: spider_crawl.py:59 -- Begin Crawling
INFO: 09-16 15:38:05: spider_crawl.py:76 -- crawling the urls : http://task.csdn.net/
, current_depth=0
ERROR: 09-16 15:38:06: spider_crawl.py:109 -- get url: http://task.csdn.net/
 faild,status_code = 404
ERROR: 09-16 15:38:06: spider_crawl.py:86 -- get url:http://task.csdn.net/
 html error
INFO: 09-16 15:38:06: spider_crawl.py:93 -- this thread crawling done!!! max_depth=1
INFO: 09-16 15:42:49: spider_crawl.py:44 -- Add the seeds url "['http://www.baidu.com\n', 'http://task.csdn.net/\n']"
INFO: 09-16 15:42:49: spider_crawl.py:59 -- Begin Crawling
INFO: 09-16 15:42:49: spider_crawl.py:77 -- crawling the urls : http://www.baidu.com
, current_depth=0
ERROR: 09-16 15:42:50: spider_crawl.py:112 -- can not connect to the http://www.baidu.com
,info:HTTPConnectionPool(host='www.baidu.com%0a', port=80): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x101758e50>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known',))
ERROR: 09-16 15:42:50: spider_crawl.py:87 -- get url:http://www.baidu.com
 html error
INFO: 09-16 15:42:50: spider_crawl.py:94 -- this thread crawling done!!! max_depth=1
INFO: 09-16 15:42:50: spider_crawl.py:59 -- Begin Crawling
INFO: 09-16 15:42:50: spider_crawl.py:77 -- crawling the urls : http://task.csdn.net/
, current_depth=0
ERROR: 09-16 15:42:51: spider_crawl.py:110 -- get url: http://task.csdn.net/
 faild,status_code = 404
ERROR: 09-16 15:42:51: spider_crawl.py:87 -- get url:http://task.csdn.net/
 html error
INFO: 09-16 15:42:51: spider_crawl.py:94 -- this thread crawling done!!! max_depth=1
INFO: 09-16 18:59:28: spider_crawl.py:44 -- Add the seeds url "['http://www.baidu.com\n', 'http://task.csdn.net/\n']"
INFO: 09-16 18:59:28: spider_crawl.py:59 -- Begin Crawling
INFO: 09-16 18:59:28: spider_crawl.py:77 -- crawling the urls : http://www.baidu.com
, current_depth=0
ERROR: 09-16 18:59:32: spider_crawl.py:112 -- can not connect to the http://www.baidu.com
,info:HTTPConnectionPool(host='www.baidu.com%0a', port=80): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x105184e90>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known',))
ERROR: 09-16 18:59:32: spider_crawl.py:87 -- get url:http://www.baidu.com
 html error
INFO: 09-16 18:59:32: spider_crawl.py:94 -- this thread crawling done!!! max_depth=1
INFO: 09-16 18:59:32: spider_crawl.py:59 -- Begin Crawling
INFO: 09-16 18:59:32: spider_crawl.py:77 -- crawling the urls : http://task.csdn.net/
, current_depth=0
ERROR: 09-16 18:59:33: spider_crawl.py:110 -- get url: http://task.csdn.net/
 faild,status_code = 404
ERROR: 09-16 18:59:33: spider_crawl.py:87 -- get url:http://task.csdn.net/
 html error
INFO: 09-16 18:59:33: spider_crawl.py:94 -- this thread crawling done!!! max_depth=1
INFO: 09-16 19:08:16: spider_crawl.py:44 -- Add the seeds url "['http://www.baidu.com\n', 'http://task.csdn.net/\n']"
INFO: 09-16 19:08:16: spider_crawl.py:80 -- crawling the urls : http://www.baidu.com
, current_depth=0
ERROR: 09-16 19:08:17: spider_crawl.py:115 -- can not connect to the http://www.baidu.com
,info:HTTPConnectionPool(host='www.baidu.com%0a', port=80): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x106dcde90>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known',))
ERROR: 09-16 19:08:17: spider_crawl.py:90 -- get url:http://www.baidu.com
 html error
INFO: 09-16 19:08:17: spider_crawl.py:97 -- this thread crawling done!!! max_depth=1
INFO: 09-16 19:08:17: spider_crawl.py:80 -- crawling the urls : http://task.csdn.net/
, current_depth=0
ERROR: 09-16 19:08:18: spider_crawl.py:113 -- get url: http://task.csdn.net/
 faild,status_code = 404
ERROR: 09-16 19:08:18: spider_crawl.py:90 -- get url:http://task.csdn.net/
 html error
INFO: 09-16 19:08:18: spider_crawl.py:97 -- this thread crawling done!!! max_depth=1
INFO: 09-16 19:08:18: spider_crawl.py:51 -- Begin Crawling
INFO: 09-16 19:08:18: spider_crawl.py:53 -- Thread-1 is start
INFO: 09-16 19:08:18: spider_crawl.py:53 -- Thread-2 is start
INFO: 09-16 19:08:18: spider_crawl.py:58 -- Thread-1 is done
INFO: 09-16 19:08:18: spider_crawl.py:58 -- Thread-2 is done
INFO: 09-16 19:13:40: spider_crawl.py:44 -- Add the seeds url "['http://task.csdn.net/   \n', 'http://www.baidu.com/']"
INFO: 09-16 19:13:40: spider_crawl.py:74 -- crawling the urls : http://task.csdn.net/   
, current_depth=0
ERROR: 09-16 19:13:41: spider_crawl.py:107 -- get url: http://task.csdn.net/   
 faild,status_code = 404
ERROR: 09-16 19:13:41: spider_crawl.py:84 -- get url:http://task.csdn.net/   
 html error
INFO: 09-16 19:13:41: spider_crawl.py:91 -- this thread crawling done!!! max_depth=1
INFO: 09-16 19:13:41: spider_crawl.py:74 -- crawling the urls : http://www.baidu.com/, current_depth=0
INFO: 09-16 19:13:42: spider_crawl.py:102 -- get http://www.baidu.com/ content successfully
INFO: 09-16 19:13:43: spider_crawl.py:79 -- get 18 new links
INFO: 09-16 19:13:43: spider_crawl.py:91 -- this thread crawling done!!! max_depth=1
INFO: 09-16 19:13:43: spider_crawl.py:51 -- Begin Crawling
INFO: 09-16 19:13:43: spider_crawl.py:53 -- Thread-1 is start
INFO: 09-16 19:13:43: spider_crawl.py:53 -- Thread-2 is start
INFO: 09-16 19:19:58: spider_crawl.py:44 -- Add the seeds url "["'http://task.csdn.net'\n", "'http://www.baidu.com'"]"
INFO: 09-16 19:19:58: spider_crawl.py:74 -- crawling the urls : 'http://task.csdn.net'
, current_depth=0
ERROR: 09-16 19:19:59: spider_crawl.py:109 -- can not connect to the 'http://task.csdn.net'
,info:No connection adapters were found for ''http://task.csdn.net'
'
ERROR: 09-16 19:19:59: spider_crawl.py:84 -- get url:'http://task.csdn.net'
 html error
INFO: 09-16 19:19:59: spider_crawl.py:91 -- this thread crawling done!!! max_depth=1
INFO: 09-16 19:19:59: spider_crawl.py:140 -- aa
INFO: 09-16 19:19:59: spider_crawl.py:74 -- crawling the urls : 'http://www.baidu.com', current_depth=0
ERROR: 09-16 19:20:00: spider_crawl.py:109 -- can not connect to the 'http://www.baidu.com',info:No connection adapters were found for ''http://www.baidu.com''
ERROR: 09-16 19:20:00: spider_crawl.py:84 -- get url:'http://www.baidu.com' html error
INFO: 09-16 19:20:00: spider_crawl.py:91 -- this thread crawling done!!! max_depth=1
INFO: 09-16 19:20:00: spider_crawl.py:140 -- aa
INFO: 09-16 19:20:00: spider_crawl.py:51 -- Begin Crawling
INFO: 09-16 19:20:00: spider_crawl.py:53 -- Thread-1 is start
INFO: 09-16 19:20:00: spider_crawl.py:53 -- Thread-2 is start
INFO: 09-16 19:26:11: spider_crawl.py:44 -- Add the seeds url "["'http://task.csdn.net'\n", "'http://www.baidu.com'"]"
INFO: 09-16 19:26:11: spider_crawl.py:75 -- crawling the urls : 'http://task.csdn.net'
, current_depth=0
ERROR: 09-16 19:26:12: spider_crawl.py:110 -- can not connect to the 'http://task.csdn.net'
,info:No connection adapters were found for ''http://task.csdn.net'
'
ERROR: 09-16 19:26:12: spider_crawl.py:85 -- get url:'http://task.csdn.net'
 html error
INFO: 09-16 19:26:12: spider_crawl.py:92 -- this thread crawling done!!! max_depth=1
INFO: 09-16 19:26:12: spider_crawl.py:75 -- crawling the urls : 'http://www.baidu.com', current_depth=0
ERROR: 09-16 19:26:13: spider_crawl.py:110 -- can not connect to the 'http://www.baidu.com',info:No connection adapters were found for ''http://www.baidu.com''
ERROR: 09-16 19:26:13: spider_crawl.py:85 -- get url:'http://www.baidu.com' html error
INFO: 09-16 19:26:13: spider_crawl.py:92 -- this thread crawling done!!! max_depth=1
INFO: 09-16 19:26:13: spider_crawl.py:52 -- Begin Crawling
INFO: 09-16 19:26:13: spider_crawl.py:54 -- Thread-1 is start
INFO: 09-16 19:26:13: spider_crawl.py:54 -- Thread-2 is start
INFO: 09-16 19:26:13: spider_crawl.py:59 -- Thread-1 is done
INFO: 09-16 19:26:13: spider_crawl.py:59 -- Thread-2 is done
INFO: 09-16 19:28:00: spider_crawl.py:44 -- Add the seeds url "["'http://task.csdn.net'\n", "'http://www.baidu.com'"]"
INFO: 09-16 19:28:21: spider_crawl.py:44 -- Add the seeds url "["'http://task.csdn.net'\n", "'http://www.baidu.com'"]"
INFO: 09-16 19:28:21: spider_crawl.py:75 -- crawling the urls : 'http://task.csdn.net'
, current_depth=0
ERROR: 09-16 19:28:22: spider_crawl.py:110 -- can not connect to the 'http://task.csdn.net'
,info:No connection adapters were found for ''http://task.csdn.net'
'
ERROR: 09-16 19:28:22: spider_crawl.py:85 -- get url:'http://task.csdn.net'
 html error
INFO: 09-16 19:28:22: spider_crawl.py:92 -- this thread crawling done!!! max_depth=1
INFO: 09-16 19:28:22: spider_crawl.py:75 -- crawling the urls : 'http://www.baidu.com', current_depth=0
ERROR: 09-16 19:28:23: spider_crawl.py:110 -- can not connect to the 'http://www.baidu.com',info:No connection adapters were found for ''http://www.baidu.com''
ERROR: 09-16 19:28:23: spider_crawl.py:85 -- get url:'http://www.baidu.com' html error
INFO: 09-16 19:28:23: spider_crawl.py:92 -- this thread crawling done!!! max_depth=1
INFO: 09-16 19:28:23: spider_crawl.py:52 -- Begin Crawling
INFO: 09-16 19:28:23: spider_crawl.py:54 -- Thread-1 is start
INFO: 09-16 19:28:23: spider_crawl.py:54 -- Thread-2 is start
INFO: 09-16 19:28:23: spider_crawl.py:59 -- Thread-1 is done
INFO: 09-16 19:28:23: spider_crawl.py:59 -- Thread-2 is done
INFO: 09-16 19:28:56: spider_crawl.py:44 -- Add the seeds url "["'http://task.csdn.net'\n", "'http://www.baidu.com'"]"
INFO: 09-16 19:28:56: spider_crawl.py:52 -- Begin Crawling
INFO: 09-16 19:28:56: spider_crawl.py:54 -- Thread-1 is start
INFO: 09-16 19:28:56: spider_crawl.py:54 -- Thread-2 is start
INFO: 09-16 19:28:56: spider_crawl.py:59 -- Thread-1 is done
INFO: 09-16 19:28:56: spider_crawl.py:59 -- Thread-2 is done
INFO: 09-16 19:38:35: spider_crawl.py:44 -- Add the seeds url "["'http://task.csdn.net'\n", "'http://www.baidu.com'"]"
INFO: 09-16 19:38:35: spider_crawl.py:52 -- Begin Crawling
INFO: 09-16 19:38:35: spider_crawl.py:54 -- Thread-1 is start
INFO: 09-16 19:38:35: spider_crawl.py:54 -- Thread-2 is start
INFO: 09-16 19:38:35: spider_crawl.py:59 -- Thread-1 is done
INFO: 09-16 19:38:35: spider_crawl.py:59 -- Thread-2 is done
INFO: 09-16 19:38:37: spider_crawl.py:44 -- Add the seeds url "["'http://task.csdn.net'\n", "'http://www.baidu.com'"]"
INFO: 09-16 19:38:37: spider_crawl.py:52 -- Begin Crawling
INFO: 09-16 19:38:37: spider_crawl.py:54 -- Thread-1 is start
INFO: 09-16 19:38:37: spider_crawl.py:54 -- Thread-2 is start
INFO: 09-16 19:38:37: spider_crawl.py:59 -- Thread-1 is done
INFO: 09-16 19:38:37: spider_crawl.py:59 -- Thread-2 is done
INFO: 09-16 19:38:57: spider_crawl.py:44 -- Add the seeds url "["'http://task.csdn.net'\n", "'http://www.baidu.com'"]"
INFO: 09-16 19:38:57: spider_crawl.py:52 -- Begin Crawling
INFO: 09-16 19:38:57: spider_crawl.py:54 -- Thread-1 is start
INFO: 09-16 19:38:57: spider_crawl.py:75 -- crawling the urls : 'http://task.csdn.net'
, current_depth=0
INFO: 09-16 19:38:57: spider_crawl.py:54 -- Thread-2 is start
INFO: 09-16 19:38:57: spider_crawl.py:75 -- crawling the urls : 'http://www.baidu.com', current_depth=0
ERROR: 09-16 19:38:58: spider_crawl.py:110 -- can not connect to the 'http://task.csdn.net'
,info:No connection adapters were found for ''http://task.csdn.net'
'
ERROR: 09-16 19:38:58: spider_crawl.py:85 -- get url:'http://task.csdn.net'
 html error
INFO: 09-16 19:38:58: spider_crawl.py:92 -- this thread crawling done!!! max_depth=1
ERROR: 09-16 19:38:58: spider_crawl.py:110 -- can not connect to the 'http://www.baidu.com',info:No connection adapters were found for ''http://www.baidu.com''
ERROR: 09-16 19:38:58: spider_crawl.py:85 -- get url:'http://www.baidu.com' html error
INFO: 09-16 19:38:58: spider_crawl.py:92 -- this thread crawling done!!! max_depth=1
INFO: 09-16 19:38:58: spider_crawl.py:59 -- Thread-1 is done
INFO: 09-16 19:38:58: spider_crawl.py:59 -- Thread-2 is done
INFO: 09-16 19:39:35: spider_crawl.py:44 -- Add the seeds url "['http://task.csdn.net\n', 'http://www.baidu.com']"
INFO: 09-16 19:39:35: spider_crawl.py:52 -- Begin Crawling
INFO: 09-16 19:39:35: spider_crawl.py:54 -- Thread-1 is start
INFO: 09-16 19:39:35: spider_crawl.py:75 -- crawling the urls : http://task.csdn.net
, current_depth=0
INFO: 09-16 19:39:35: spider_crawl.py:54 -- Thread-2 is start
INFO: 09-16 19:39:35: spider_crawl.py:75 -- crawling the urls : http://www.baidu.com, current_depth=0
ERROR: 09-16 19:39:39: spider_crawl.py:110 -- can not connect to the http://task.csdn.net
,info:HTTPConnectionPool(host='task.csdn.net%0a', port=80): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x104d525d0>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known',))
ERROR: 09-16 19:39:39: spider_crawl.py:85 -- get url:http://task.csdn.net
 html error
INFO: 09-16 19:39:39: spider_crawl.py:92 -- this thread crawling done!!! max_depth=1
INFO: 09-16 19:39:39: spider_crawl.py:59 -- Thread-1 is done
INFO: 09-16 19:39:39: spider_crawl.py:103 -- get http://www.baidu.com content successfully
INFO: 09-16 19:39:39: spider_crawl.py:80 -- get 18 new links
INFO: 09-16 19:39:39: spider_crawl.py:92 -- this thread crawling done!!! max_depth=1
INFO: 09-16 19:39:39: spider_crawl.py:59 -- Thread-2 is done
INFO: 09-16 19:40:39: spider_crawl.py:44 -- Add the seeds url "['http://task.csdn.net/\n', 'http://www.baidu.com/']"
INFO: 09-16 19:40:39: spider_crawl.py:52 -- Begin Crawling
INFO: 09-16 19:40:39: spider_crawl.py:54 -- Thread-1 is start
INFO: 09-16 19:40:39: spider_crawl.py:75 -- crawling the urls : http://task.csdn.net/
, current_depth=0
INFO: 09-16 19:40:39: spider_crawl.py:54 -- Thread-2 is start
INFO: 09-16 19:40:39: spider_crawl.py:75 -- crawling the urls : http://www.baidu.com/, current_depth=0
INFO: 09-16 19:40:40: spider_crawl.py:103 -- get http://www.baidu.com/ content successfully
ERROR: 09-16 19:40:40: spider_crawl.py:108 -- get url: http://task.csdn.net/
 faild,status_code = 404
ERROR: 09-16 19:40:40: spider_crawl.py:85 -- get url:http://task.csdn.net/
 html error
INFO: 09-16 19:40:40: spider_crawl.py:92 -- this thread crawling done!!! max_depth=1
INFO: 09-16 19:40:40: spider_crawl.py:59 -- Thread-1 is done
INFO: 09-16 19:40:41: spider_crawl.py:80 -- get 18 new links
INFO: 09-16 19:40:41: spider_crawl.py:92 -- this thread crawling done!!! max_depth=1
INFO: 09-16 19:40:41: spider_crawl.py:59 -- Thread-2 is done
INFO: 09-16 19:41:31: spider_crawl.py:44 -- Add the seeds url "['http://task.csdn.net/']"
INFO: 09-16 19:41:31: spider_crawl.py:52 -- Begin Crawling
INFO: 09-16 19:41:31: spider_crawl.py:54 -- Thread-1 is start
INFO: 09-16 19:41:31: spider_crawl.py:75 -- crawling the urls : http://task.csdn.net/, current_depth=0
INFO: 09-16 19:41:32: spider_crawl.py:103 -- get http://task.csdn.net/ content successfully
INFO: 09-16 19:41:32: spider_crawl.py:80 -- get 2 new links
INFO: 09-16 19:41:32: spider_crawl.py:92 -- this thread crawling done!!! max_depth=1
INFO: 09-16 19:41:32: spider_crawl.py:59 -- Thread-1 is done
INFO: 09-16 19:43:59: spider_crawl.py:45 -- Add the seeds url "['http://task.csdn.net/', 'http://www.baidu.com/']"
INFO: 09-16 19:43:59: spider_crawl.py:53 -- Begin Crawling
INFO: 09-16 19:43:59: spider_crawl.py:55 -- Thread-1 is start
INFO: 09-16 19:43:59: spider_crawl.py:76 -- crawling the urls : http://task.csdn.net/, current_depth=0
INFO: 09-16 19:43:59: spider_crawl.py:55 -- Thread-2 is start
INFO: 09-16 19:43:59: spider_crawl.py:76 -- crawling the urls : http://www.baidu.com/, current_depth=0
INFO: 09-16 19:44:00: spider_crawl.py:104 -- get http://task.csdn.net/ content successfully
INFO: 09-16 19:44:00: spider_crawl.py:104 -- get http://www.baidu.com/ content successfully
INFO: 09-16 19:44:00: spider_crawl.py:81 -- get 2 new links
INFO: 09-16 19:44:00: spider_crawl.py:93 -- this thread crawling done!!! max_depth=1
INFO: 09-16 19:44:00: spider_crawl.py:60 -- Thread-1 is done
INFO: 09-16 19:44:01: spider_crawl.py:81 -- get 18 new links
INFO: 09-16 19:44:01: spider_crawl.py:93 -- this thread crawling done!!! max_depth=1
INFO: 09-16 19:44:01: spider_crawl.py:60 -- Thread-2 is done
